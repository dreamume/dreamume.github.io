---
layout:     post
title:      "Weekly 087"
subtitle:   "Algorithm: Tallest Billboard; Review: Rebuilding Netflix Video Processing Pipeline with Microservices; Tips: System Design Interview - Notification Service; Share: Welcome to The Mobile Interview"
thumbnail-img: ""
date:       2024-01-12 21:10
author:     "dreamume"
tags: 		[it]
category:   it
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# Table of Contents

1.  [Algorithm](#orga96aa88)
2.  [Review](#org3097946)
    1.  [从 Reloaded 到 Cosmos](#orgb2bb611)
        1.  [重加载](#org8036a36)
        2.  [Cosmos](#org86d4f76)
    2.  [在 Cosmos 中构建视频处理流水线](#orgda1d981)
        1.  [服务边界](#org5a19de7)
3.  [Tips](#org4edda99)
4.  [Share](#org9f8f036)


<a id="orga96aa88"></a>

# Algorithm

Leetcode 956: [Tallest Billboard](https://leetcode.com/problems/tallest-billboard/)

<https://dreamume.medium.com/leetcode-956-tallest-billboard-1a37590b7f0c>


<a id="org3097946"></a>

# Review

[Rebuilding Netflix Video Processing Pipeline with Microservices](https://netflixtechblog.com/rebuilding-netflix-video-processing-pipeline-with-microservices-4e5e6310e359)

这是 Netflix 如何用微服务重建它的视频处理流水线系列文章的第一篇，所以我们可维护我们小步快速革新和对成员流和视频操作持续改进系统。本篇介绍文章聚焦于整体的概述。后续的文章将提供更深入到每个服务、共享前景和课程学习

Netflix 视频处理流水线从 2007 年在直播中启动我们的流服务。从此，视频流水线的运行有了显著的改进和更广的扩展：

-   在标准定义上从标准动态范围（SDR）开始，我们扩展编码流水线到 4K 和高动态范围（HDR），其启动支持我们的优质提供
-   我们从集中线性编码到分布式块编码。这个架构改动极大地缩减了处理延时和增加了系统适应性
-   从被质量限制的修饰实例移除，我们采用 Netflix 由自动扩展微服务创建内部低点，使得计算弹性和资料利用率有了重要的改进
-   我们首次发布革新比如每标题和每影片优化，对 Netflix 成员提供重要的质量体验（QoE）改进
-   通过集成操作内容系统，我们启动流水线从创新性和创建更紧密成员体验比如交互式故事叙述来杠杆富元数据
-   我们扩展流水线支持来服务我们的操作/内容开发用户事例，其有不同的延时和适应性需求相比较传统的流用户事例

我们最近十多年经验加强了我们的确信，一个高效、灵活的视频处理流水线允许我们革新和支持我们的流服务，和我们的操作合作者，对 Netflix 持续成功是重要的。最后，视频和图像编码团队在编码技术（ET）用最近几年重建了视频处理流水线在我们的下一代基于微服务的技术平台 Cosmos 上


<a id="orgb2bb611"></a>

## 从 Reloaded 到 Cosmos


<a id="org8036a36"></a>

### 重加载

从 2014 年开始，我们在我们的第三代平台 Reloaded 上发展并操作视频处理流水线。Reloaded 是良好架构、提供好的稳定性、扩展性和灵活性的合理水平。它被我们的团队发展作为各种编码革新的基础

当 Reloaded 被设计时，我们聚焦在单个用户用例：转换从操作室接收到的高质量媒体文件压缩为 Netflix 流资产。Reloaded 作为单个单调增长系统被创建，ET 中各个媒体团队开发者和我们的平台合作方内容架构和解决方案（CIS）工作在相同的代码上，构建单个系统处理所有媒体资产。数年后，系统扩展支持各种新的用户用例。这导致系统复杂度显著增长，且 Reloaded 的限制开始显示：

-   耦合的功能：Reloaded 由一组工作模块和一个协作模块组成。新的 Reloaded 模块建立和对写作模块的集成需要化不小的功夫，导致对参数而不是开发新功能创建的偏见。例如，在 Reloaded 中视频质量计算实现在视频编码模块内部。对这个实现，它在不重编码下很难重计算视频质量
-   单调增长结构：因为 Reloaded 模块经常在相同仓库下重定位，它容易忽视代码隔离规则且有跨强边界的不想要重用的代码。这样的重用导致高耦合且减慢开发速度。模块间高耦合进一步使得我们把所有模块弄在一起
-   长发布周期：开发节点意味着对这个开发规模调试和回滚很困难，增长了对不想要的产品出现运行终端的恐惧。这驱使了发布训练处理。每两周，制作一个所有模块的快照，并晋升为发布替补。这个发布替补然后执行全面地测试来尝试覆盖尽可能多的表面问题。这个测试阶段花费两周。这样，取决于代码什么时候合并，它将耗费两到四周来达到发布状态

随着时间和功能的增长，在 Reloaded 中新特性贡献的速度下降。一些承诺的想法由于过大的工作需要克服架构限制而放弃。平台曾经服务很好但现在发展落后了


<a id="org86d4f76"></a>

### Cosmos

作为响应，在 2018 年 CIS 和 ET 团队开始开发下一代平台，Cosmos。对于已经加入 Reloaded 的开发者的扩展性和稳定性，Cosmos 目标是重要增长系统的灵活性和特性开发速度。为达到这些，Cosmos 作为工作流，媒体中心微服务驱动的计算平台来开发

微服务架构在服务之间提供强解偶。每个微服务工作流支持简化实现复杂媒体工作流逻辑。最后，相关抽象允许媒体算法开发者聚焦于音视频信号的操作而不是担心基础架构。一个 Cosmos 提供的好处的可理解列表可在 [博客](https://netflixtechblog.com/the-netflix-cosmos-platform-35c14d9351ad) 中查看


<a id="orgda1d981"></a>

## 在 Cosmos 中构建视频处理流水线


<a id="org5a19de7"></a>

### 服务边界

在微服务架构中，一个系统由一系列服务组成，每个服务聚焦于单个功能。这样第一件事（最重要的）是确定边界和定义服务

在我们的流水线，媒体资产从创建到吸收到转发，经过一些处理步骤比如分析和转换。我们分析这些处理步骤来确定边界和把它们分组到不同的域，这也就变成我们用的微服务的构建块

举一个例子，在 Reloaded，视频编码模块捆绑了 5 个步骤：

1.  分割输入视频为小块
2.  独立编码每个小块
3.  计算每个小块的质量分数（VMAF）
4.  组装所有编码块为一个单独编码视频
5.  总计所有块的质量分数

从一个系统来看，组装编码视频由于存在在内部分块和分割块编码为了填满某种延迟和适应性需求而成为主要忧虑。进一步，包括以上所有的，视频质量计算提供一个总的分离功能作为对编码服务的比较

这样，在 Cosmos，我们创建两个独立微服务：视频编码服务（VES）和视频质量服务（VQS），每个都是清晰解偶的功能。作为实现细节，块编码和组装被抽象放入 VES


<a id="org4edda99"></a>

# Tips

[System Design Interview - Notification Service](https://www.youtube.com/watch?v=bBTPZ9NdSk8)

功能性需求：

-   createTopic(topicName)
-   publish(topicName, message)
-   subscribe(topicName, endpoint)

非功能性需求：

-   可扩展
-   高可用
-   高性能
-   耐用性

对于高层架构，请求要平均分布到服务器，处理这个初始请求的组件是前端服务。我们将使用数据库存储话题和订阅信息。我们将把数据库隐藏在另一个微服务之后，元数据服务。这样做的原因有几个：首先，考虑隔离，一个设计原则告诉我们提供接口访问到数据库，这极大简化将来维护和进行改变的能力。其次，元数据服务将作为数据库和其他组件之间的一个缓存层。我们不想要每个消息发布到系统时连接数据库。我们想要从缓存中提取话题元数据。其次，我们需要存储一定时间的消息。这个时间段将很短如果所有订阅者有效且消息成功发送到所有节点。或我们可需要存储消息稍微长点时间（几天），这样消息可之后被提取如果一些订阅者当前无效。我们需要的另一个组件是从消息存储中提取消息并发送到订阅者。发送者也需要调用元数据服务来提取订阅者的信息。当创建话题和订阅 API 被调用，我们需要存储所有这些信息到数据库

前端服务：

-   一个轻量级 Web 服务
-   跨几个数据中心采用的无状态服务

动作：

-   请求有效性
-   验证/授权
-   TLS（SSL）终止
-   服务端加密
-   缓存
-   限速
-   请求分发
-   请求去重
-   使用数据收集

前端服务的第一个组件是反向代理，其是一个轻量级服务器。其作用比如 SSL 终结，当 HTTPS 请求过来时解密且发送非加密的形式。同时，代理负责加密发送到客户端的响应。第二个职责是压缩（例如用 gzip），代理压缩要返回给客户端的响应。另一个代理的功能是处理服务下降，我们可返回 503 HTTP 状态码（服务无效）如果前端服务变慢或完全不可用。反向代理然后传递请求到前端 Web 服务。对每个已发布的消息，前端服务需要调用元数据服务来获得消息话题的信息。为减少对元数据服务的调用，前端服务可使用本地缓存。我们可使用一些有名的缓存实现（比如 Google Guava）或创建我们自己的 LRU 缓存实现。前端服务也写一堆不同的日志。我们必须记录服务健康信息，记录服务发生的异常，记录度量信息。这是一种键值对之后会总计用来监控服务健康和统计。例如，请求数，故障，调用延时，我们将需要所有这样的信息来监控系统。我们可能也需要写信息用来审查，例如记录谁和什么时候生成请求到系统中一个特殊的 API。实际的日志数据处理由其他组件管理，通常称为代理。代理负责数据聚集和传输日志到其他系统，为后处理和存储。这个职责的分离使得前端服务简化，更快和更健壮

通知系统的另一个组件是元数据服务，负责存储话题和订阅信息到数据库：

-   在前端服务和持久化存储之间一个分布式缓存层
    
    每个前端主机计算一个哈希，例如 MD5 哈希，使用一些键，例如话题名和话题所有者唯一识别码的组合。基于这个哈希前端主机拿到对应的元数据服务主机。这里一个选项是引入一个组件负责协调。这个组件知道所有元数据服务主机，这些主机发送心跳到该组件。每个前端主机向配置服务询问哪个元数据服务主机包含特殊的哈希值。每次我们扩展且添加更多元数据服务主机，配置服务知道这个改变并重映射哈希键范围。另一个选项是我们不使用任何协调器。我们确保每个前端主机可包含所有元数据服务主机的信息。当更多元数据服务主机添加或由于硬件故障死亡时每个前端主机都会被通知。存在不同的机制帮助前端主机发现元数据服务主机。比如可用 Gossip 协议
-   多读少写


<a id="org9f8f036"></a>

# Share

