---
layout:     post
title:      "Weekly 062"
subtitle:   "Algorithm: ; Review: Notes about Media Technology; Tips: Notes about Media Technology in mobile platform; Share:"
thumbnail-img: ""
date:       2022-08-08 20:00
author:     "dreamume"
tags: 		[it]
category:   it
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# Table of Contents

1.  [Algorithm](#orgc2e6ec3)
2.  [Review](#org8075d53)
    1.  [如何做音视频的封装与转码](#orgb6d52a4)
        1.  [封装容器格式：MP4](#org7db09a1)
3.  [Tips](#org7497310)
    1.  [iOS 平台音频渲染（一）：使用 AudioQueue 渲染音频](#org6d5da76)
        1.  [AVAudioSession](#org652fc80)
        2.  [AudioQueue 详解](#orge1a3583)
        3.  [AudioQueue 运行流程](#org5b25c36)
        4.  [iOS 平台的音频格式](#org1eb44eb)
    2.  [iOS 平台音频渲染（二）：使用 AudioUnit 渲染音频](#orgcb666de)
        1.  [AudioUnit 的分类](#orgfb5f561)
        2.  [创建 AudioUnit](#orgffb4305)
        3.  [RemoteIO 详解](#org7a86039)
        4.  [小结](#org5683819)
4.  [Share](#org33b8af6)


<a id="orgc2e6ec3"></a>

# Algorithm


<a id="org8075d53"></a>

# Review

音视频技术入门课    刘岐


<a id="orgb6d52a4"></a>

## 如何做音视频的封装与转码

视频转码主要涉及编码压缩算法（Encoding）、格式封装操作（Muxing）、数据传输（例如 RTMP、RTP）、格式解封装（Demuxing）、解码解压算法（Decoding）几方面的操作。这些操作需要一个共识的协定，所以通常音视频技术都会有固定的参考标准

标准中没有写明的，通常兼容性不好。所以在我们将视频流、音频流写入到一个封装容器中之前，需要先弄清楚这个容器是否支持我们当前的视频流、音频流数据


<a id="org7db09a1"></a>

### 封装容器格式：MP4

MP4 格式标准为 ISO-14496 Part 12、ISO-14496 Part 14，标准内容并不是特别多，如果要了解 MP4 的格式信息，我们首先要清楚几个概念：

1.  MP4 文件由许多 Box 和 FullBox 组成
2.  每个 Box 由 Header 和 Data 两部分组成
3.  FullBox 则是 Box 的扩展，在 Box 结构的基础上，在 Header 中增加 8bit 位 version 标志和 24bit 位的 flags 标志
4.  Header 包含了整个 Box 的长度的大小（Size）和类型（Type），当 Size 等于 0 时，代表这个 Box 是文件中的最后一个 Box；当 Size 等于 1 时说明 Box 长度需要更多的 bits 位来描述，在后面定义一个 64bits 位的 largesize 用来描述 Box 的长度；当 Type 为 uuid 时，说明这个 Box 中的数据是用户自定义扩展类型
5.  Data 为 Box 的实际数据，可以是纯数据，也可以是更多的子 Box
6.  当一个 Box 中 Data 是一系列的子 Box 时，这个 Box 又可以称为 Container Box

MP4 文件中 Box 组成，你可以仔细阅读一下 [参考标准 ISO-14496 Part 12](http://mp4ra.org/#/references)

MP4 封装格式文件中，我们经常会遇到 moov box 与 mdat box。我们存储音频与视频数据的索引信息，就是存在 moov box 中，音频和视频的索引信息在 moov 中分别存储在不同的 trak 里面，trak 里面会保存对应的数据采样相关的索引信息，通过获得 moov 中的索引信息之后，根据索引信息我们才能从 mdat 中读取音视频数据，所以 MP4 文件中必不可少的是 moov 信息，如果缺少 moov 信息的话，这个点播文件将无法被成功打开

![img](../img/mp4_file_format_common_boxes.webp)

关于 MP4 封装里面能否存放我们当前想存的 codec，还需要查找一下参考标准或者公共约定的标准，看看是否允许存放，你可以参考 [MP4 网站](http://mp4ra.org/#/codecs)

moov 一般在 MP4 文件的头部，一般需要先写入 mdat 中的音视频数据，这样我们就知道数据采样存储的位置和大小，然后才能写入到 moov 中，解决办法是生成文件之后做一个后处理，也就是将 moov 移动到 mdat 前面


<a id="org7497310"></a>

# Tips

移动端音视频开发实战    展晓凯


<a id="org6d5da76"></a>

## iOS 平台音频渲染（一）：使用 AudioQueue 渲染音频


<a id="org652fc80"></a>

### AVAudioSession

它以单例的形式存在，用于管理与获取 iOS 设备音频的硬件信息

1.  基本设置

    1.  根据我们需要硬件设备提供的能力来设置类别
        
            [audioSession setCategory:AVAudioSessionCategoryPlayback error:&error];
    
    2.  设置 I/O 的 Buffer，Buffer 越小说明延迟越低
        
            NSTimeInterval bufferDuration = 0.002;
            [audioSession setPreferredIOBufferDuration:bufferDuration error:&error];
    
    3.  设置采样频率，让硬件设备按照设置的采样率来采集或者播放音频
        
            double hwSampleRate = 44100.0;
            [audioSession setPreferredSampleRate:hwSampleRate error:&error];
    
    4.  当设置完毕所有参数之后就可以激活 AudioSession 了
        
            [audioSession setActive:YES error:&error];

2.  深入理解 AudioSession

    关于 Category 和 CategoryOptions
    
    1.  Category 是向系统描述应用需要的能力，常用的分类如下
        1.  AVAudioSessionCategoryPlayback：用于播放录制音乐或者其他声音的类别，如果要在应用程序转换到后台时继续播放（锁屏情况下），在 Xcode 中设置 UIBackgroundModes 即可。默认情况下，使用此类别意味着，应用的音频不可混合，激活音频会话将中断其他不可混合的音频会话。如果使用混音，则使用 AVAudioSessionCategoryOptionMixWithOthers
        2.  AVAudioSessionCategoryPlayAndRecord：同时需要录音（输入）和播放（输出）音频的类别，例如 K 歌、RTC 场景。注意：用户必须打开音频录制权限（iPhone 麦克风权限）
    2.  CategoryOptions 是向系统设置类别的可选项，具体分类如下
        1.  AVAudioSessionCategoryOptionDefaultToSpeaker：此选项只能在使用 PlayAndRecord 类别时设置。它用于保证在没有使用其他配件（如耳机）的情况下，音频始终会路由至扬声器而不是听筒。而如果类别设置的是 Playback，系统会自动使用 Speaker 进行输出，无需进行此项设置
        2.  AVAudioSessionCategoryOptionAllowBluetooth：此选项代表音频录入和输出全部走蓝牙设备，仅可以为 PlayAndRecord 还有 Record 这两个类别设置这个选项，注意此时播放和录制的声音音质均为通话音质（16kHz），适用于 RTC 的通话场景，但不适用于 K 歌等需要高音质采集与播放的场景
        3.  AVAudioSessionCategoryOptionAllowBluetoothA2DP：此选项代表音频可以输出到高音质（立体声、仅支持音频输出不支持音频录入）的蓝牙设备中。如果使用 Playback 类别，系统将自动使用这个 A2DP 选项，如果使用 PlayAndRecord 类别，需要开发者自己手动设置这个选项，音频采集将使用机身内置麦克风（在需要高音质输入输出场景下可以设置成这个）
        4.  监听音频焦点抢占，一般在检测到音频被打断的时候处理一些自己业务上的操作，比如暂停播放音频等，代码如下
            
                [[NSNotificationCenter defaultCenter] addObserver:self 
                                                         selector:@selector(audioSessionInterruptionNoti:)
                                                             name:AVAudioSessionInterruptionNotification
                                                           object:[AVAudioSession sharedInstance]];
                
                - (void)audioSessionInterruptionNoti:(NSNotification *)noti {
                  AVAudioSessionInterruptionType type = 
                    [noti.userInfo[AVAudioSessionInterruptionTypeKey] intValue];
                  if (type == AVAudioSessionInterruptionTypeBegan) {
                    // do something
                  }
                }
        5.  监听声音硬件路由变化，当检测到插拔耳机或者接入蓝牙设备的时候，业务需要做一些自己的操作，代码如下
            
                [[NSNotificationCenter defaultCenter] addObserver:self 
                                                         selector:@selector(audioRouteChangeListenerCallback:)
                                                             name:AVAudioSessionRouteChangeNotification
                                                           object:nil];
                
                - (void)audioRouteChangeListenerCallback:(NSNotification *)notification {
                  NSDictionary* interruptionDict = notification.userInfo;
                  NSInteger routeChangeReason = 
                    [[interruptionDict valueForKey:AVAudioSessionRouteChangeReasonKey] intergerValue];
                  if (routeChangeReason == AVAudioSessionRouteChangeReasonCategoryChange ||
                      routeChangeReason == AVAudioSessionRouteChangeReasonCategoryOverride) {
                    // do something
                  }
                }
        6.  申请录音权限，首先判断授权状态，如果没有询问过，就询问用户授权，如果拒绝了就引导用户进入设置页面手动打开
            
                AVAuthorizationStatus status = 
                  [AVCaptureDevice authorizationStatusForMediaType:AVMediaTypeAudio];
                if (status == AVAuthorizationStatusNotDetermined) {
                  [[AVAudioSession sharedInstance] requestRecordPermission:^(BOOL granted) {
                    }];
                 } else if (status == AVAuthorizationStatusNotRestricted || 
                            status == AVAuthorizationStatusDenied) {
                  // 引导用户跳入设置页面
                 } else {
                  // 已授权
                 }
    
    注意从 iOS 10 开始，所有访问任何设备的应用都必须静态声明其意图。为此，应用程序现在必须在其 Info.plist 文件中包含 NSMicrophoneUsageDescription 键，并为此密钥提供目的字符串


<a id="orge1a3583"></a>

### AudioQueue 详解

iOS 为开发者在 AudioToolbox 这个 framework 中提供了一个名为 AudioQueueRef 的类，AudioQueue 内部会完成以下职责：

-   连接音频的硬件进行录音或者播放
-   管理内存
-   根据开发者配置的格式，调用编解码器进行音频格式转换

AudioQueue 暴露给开发者的接口如下

-   使用正确的音频格式、回调方法等参数，创建出 AudioQueueRef 对象
-   为 AudioQueueRef 分配 Buffer，并将 Buffer 入队，启动 AudioQueue
-   在 AudioQueueRef 的回调中填充指定格式的音频数据，并且重新入队
-   暂停、恢复等常规接口


<a id="org5b25c36"></a>

### AudioQueue 运行流程

分为启动和运行阶段，启动阶段主要是应用程序配置和启动 AudioQueue；运行阶段主要是 AudioQueue 开始播放之后回调给应用程序填充 buffer，并重新入队，3 个 buffer 周而复始地运行起来；直到应用程序调用 AudioQueue 的 Pause 或者 Stop 等接口

1.  启动阶段

    1.  配置 AudioQueue
        
            AudioQueueNewInput(&dataformat, playCallback, (__bridge void *)self, 
                               NULL, NULL, 0, &queueRef);
        
        dataformat 就是音频格式，函数返回值如果为 noErr 则说明配置成功
    
    2.  分配 3 个 Buffer，并且依次灌到 AudioQueue 中
        
            for (int i = 0; i < kNumberBuffers; ++i) {
              AudioQueueAllocateBuffer(queueRef, bufferBytesSize, &buffers[i]);
              AudioQueueEnqueueBuffer(queueRef, buffers[i], 0, NULL);
             }
    
    3.  调用 Play 方法进行播放
        
            AudioQueueStart(queueRef, NULL);

2.  运行阶段

    1.  AudioQueue 启动之后会播放第一个 buffer
    2.  当播放完第一个 buffer 之后，会继续播放第二个 buffer，但是与此同时将第一个 buffer 回调给业务层由开发者进行填充，填充完毕重新入队
    3.  第二个 buffer 播放完毕后，会继续播放第三个 buffer，与此同时会将第二个 buffer 回调给业务层由开发者进行填充，填充完毕重新入队
    4.  第三个 buffer 播放完毕后，会继续循环播放队列中的第一个 buffer，也会将第三个 buffer 回调给业务层由开发者进行填充，填充完毕重新入队
    
        static void playCallback(void *aqData, AudioQueueRef inAQ, AudioQueueRef inBuffer) {
          KSAudioPlayer *player = (__bridge KSAudioPlayer *)aqData;
          // fill data
          AudioQueueEnqueueBuffer(player->queueRef, inBuffer, numPackets, player.mPacketDescs);
        }

3.  AudioQueue 中 Codec 运行流程

    1.  开发者配置 AudioQueue 的时候告诉 AudioQueue 具体编码格式
    2.  开发者在回调函数中按照原始格式填充 buffer
    3.  AudioQueue 会自己采用合适的 Codec 将压缩数据解码成 PCM 进行播放


<a id="org1eb44eb"></a>

### iOS 平台的音频格式

音频格式 ASBD（AudioSessionBasicDescription），以下对一些字段进行解释

-   mFormatID 这个参数用来指定音频的编码格式，此处音频编码格式指定为 PCM 格式
-   mFormatFlags 是用来描述声音表示格式的参数，代码中的第一个参数指定每个 sample 的表示格式是 Float 格式。这个类似于我们之前讲解的每个 sample 使用两个字节（Sint16）来表示；然后后面的参数 NonInterleaved，如果指定 NonInterleaved，那么左声道在 mBuffers[0] 里，右声道在 mBuffers[1] 里，如果是 Interleaved，左右声道数据交错排列在 mBuffers[0] 中
-   mBitsPerChannel 表示一个声道的音频数据用多少位来表示，我们知道采样使用 Float 来表示，所以这里就使用 8 乘以每个采样的字节数来赋值
-   最后是参数 mBytesPerFrame 和 mBytesPerPacket，如果是 NonInterleaved，就赋值为 bytesPerSample，否则为 bytesPerSample \* channels

如果要播放一个 MP3 或 M4A 文件，如下代码设置 ASBD

    NSURL *fileURL = [NSURL URLWithString:filePath];
    OSStatus status = 
      AudioFileOpenURL((__bridge CFURLRef)fileURL, kAudioFileReadPermission,
                       kAudioFileCAFType, &_mAudioFile);
    if (status != noErr) {
      NSLog(@"open file error");
     }
    
    // 获取文件格式
    UInt32 dataFormatSize = sizeof(dataFormat);
    AudioFileGetProperty(_mAudioFile, kAudioFilePropertyDataFormat, 
                         &dataFormatSize, &dataFormat);


<a id="orgcb666de"></a>

## iOS 平台音频渲染（二）：使用 AudioUnit 渲染音频

在以下场景中，更适合使用 AudioUnit，而不是高层次的音频框架

-   VOIP 场景下，想使用低延迟的音频 I/O
-   合成多路声音并且回放，比如游戏或者音乐合成器（弹唱、多轨乐器）的应用
-   使用 AudioUnit 里特有的功能，比如：均衡器、压缩器、混响器等效果器，以及回声消除、Mix 两轨音频等
-   需要图状结构来处理音频时，可以使用 iOS 提供的 AUGraph 和 AVAudioEngine 的 API 接口，把音频处理模块组装到灵活的图状结构中


<a id="orgfb5f561"></a>

### AudioUnit 的分类

iOS 根据 AudioUnit 的功能不同，将 AudioUnit 分成了 5 大类

-   Effect Unit
    
    第一个类型是 kAudioUnitType_Effect，主要提供声音特效处理的功能。子类型及用途如下：
    
    -   均衡效果器：子类型是 kAudioUnitSubType_NBandEQ，主要作用是给声音的某一些频带增强或者减弱能量，这个效果器需要指定多个频带，然后为每个频带设置宽度以及增益，最终将改变声音在频域上的能量分布
    -   压缩效果器：子类型是 kAudioUnitSubType_DynamicsProcessor，主要作用是当声音较小的时候可以提高声音的能量，当声音能量超过了设置的阙值，可以降低声音的能量，当然我们要设置合适的作用时间和释放时间以及触发值，最终可以将声音在时域上的能量压缩到一定范围之内
    -   混响效果器：子类型是 kAudioUnitSubType_Reverb2，是对人声处理非常重要的效果器，可以想象我们在一个空房子中，有非常多的反射声和原始声音叠加在一起，可以从听感上会更有震撼力，但是同时也会使原始声音更加模糊，遮盖掉原始声音的一些细节，所以混响设置得大或小对不同的人来讲非常不一致，可以根据自己的喜好来设置
    
    这三种是常用的，当然这个大类型下面还有很多子类型的效果器，像高通（High Pass）、低通（Low Pass）、带通（Band Pass）、延迟（Delay）、压限（Limiter）等效果器

-   Mixer Units
    
    第二个大类型是 kAudioUnitType_Mixer，主要提供 Mix 多路声音的功能。子类型及用途如下：
    
    -   3D Mixer：这个效果器在移动设备上无法使用，只能在 OS X 上
    
    -   MultiChannelMixer：子类型是 kAudioUnitSubType_MultiChannelMixer，这个效果器是我们重点介绍的对象，它是多路声音混音的效果器，可以接收多路音频的输入，还可以分别调整每一路音频的增益与开关，并将多路音频合并成一路

-   I/O Units
    
    第三个大类型是 kAudioUnitType_Output，它的用途就像它分类的名字一样，主要提供的就是 I/O 功能
    
    -   RemoteIO：子类型是 kAudioUnitSubType_RemoteIO，用来采集与播放音频
    
    -   Generic Output：子类型是 kAudioUnitSubType_GenericOutput，当开发者需要离线处理，或者说在 AUGraph 中不使用 Speaker（扬声器）来驱动整个数据流，而是希望使用一个输出（可以放入内存队列或者进行磁盘 I/O 操作）来驱动数据流的话，就使用这个子类型

-   Format Converter Units
    
    第四个大类型是 kAudioUnitType_formatConverter，提供格式转换的功能
    
    -   AUConverter：子类型是 kAudioUnitSubType_AUConverter，某些效果器对输入的音频格式有明确的要求，比如 3D Mixer Unit 就必须使用 UInt16 格式的 sample，或者开发者将音频数据后续交给一些其他编码器处理，又或者开发者想使用 SInt16 格式的 PCM 裸数据进行其他 CPU 上音频算法计算等场景下，就需要用到这个 ConverterNode
        
        比较典型的场景是我们自定义的一个音频播放器，由 FFmpeg 解码出来的 PCM 数据是 SInt16 格式表示的，我们不可以直接让 RemoteIO Unit 播放，而是需要构建一个 ConvertNode，将 SInt16 格式表示的数据转换为 Float32 表示的数据，然后再给到 RemoteIO Unit，最终才能正常播放出来
    -   Time Pitch：子类型是 kAudioUnitSubType_NewTimePitch，即变速变调效果器，这是一个比较意思的效果器，可以对声音的音高、速度进行更改，像 Tom 猫这样的应用场景就可以使用这个效果器实现
-   Generator Units
    
    第五个大类型是 kAudioUnitType_Generator，在开发中我们经常用它来提供播放器的功能。子类型及用途说明如下：
    
    -   AudioFilePlayer：子类型是 kAudioUnitSubType_AudioFilePlayer，在 AudioUnit 里面，如果我们的输入不是麦克风，而是一个媒体文件，要怎么办呢？当然也可以自己进行解码，通过转换之后给 RemoteIO Unit 播放出来。但其实还有一种更加简单、方便的方式，那就是使用 AudioFilePlayer 这个 AudioUnit，其实数据源还是会调用 AudioFile 里面的解码功能，将媒体文件中的压缩数据解压成为 PCM 裸数据，最终再交给 AudioUnitPlayer Unit 进行后续处理
        
        这里需要注意，我们必须在 AUGraph 初始化了之后，再去配置 AudioFilePlayer 的数据源以及播放范围等属性，否则会出现错误


<a id="orgffb4305"></a>

### 创建 AudioUnit

构建 AudioUnit 时，需要指定类型（Type）、子类型（Subtype）以及厂商（Manufacture）

厂商一般情况下比较固定，直接写成 kAudioUnitManufacturer_Apple

如下我们创建一个 RemoteIO 类型的 AudioUnit：

    AudioComponentDescription ioUnitDescription;
    ioUnitDescription.componentType = kAudioUnitType_Output;
    ioUnitDescription.componentSubType = kAudioUnitSubType_RemoteIO;
    ioUnitDescription.componentManufacturer = kAudioUnitManufacturer_Apple;
    ioUnitDescription.componentFlags = 0;
    ioUnitDescription.componentFlagsMask = 0;

用这个描述构造真正的 AudioUnit 有两种方法：一种是直接使用 AudioUnit 裸的创建方式；第二种使用 AUGraph 和 AUNode （其实一个 AUNode 就是对 AudioUnit 的封装）方式来构建

1.  裸创建方式
    
    先根据 AudioUnit 描述，找出实际的 AudioUnit 类型
    
        AudioComponent ioUnitRef = AudioComponentFindNext(NULL, &ioUnitDescription);
    
    然后声明一个 AudioUnit 引用
    
        AudioUnit ioUnitInstance;
    
    最后根据类型创建出这个 AudioUnit 实例
    
        AudioComponentInstanceNew(ioUnitRef, &ioUnitInstance);

2.  AUGraph 创建方式
    
    先实例化一个 AUGraph
    
        AudioGraph processingGraph;
        NewAUGraph(processingGraph);
    
    再添加一个 AUNode
    
        AUNode ioNode;
        AUGraphAddNode(processingGraph, &ioUnitDescription, &ioNode);
    
    打开 AUGraph，这个过程中也间接实例化 AUGraph 中所有的 AUNode 的过程。注意，必须在获取 AudioUnit 之前打开整个 Graph
    
        AUGraphOpen(processingGraph);
    
    最后在 AUGraph 中的某个 Node 里面获得 AudioUnit 的引用
    
        AudioUnit ioUnit;
        AUGraphNodeInfo(processingGraph, ioNode, NULL, &ioUnit);
    
    使用 AUGraph 的结构可以在我们的应用中搭建出扩展性更高的系统


<a id="org7a86039"></a>

### RemoteIO 详解

![img](../img/remoteIO_unit.webp)

RemoteIO Unit 分为 Element0 和 Element1，其中 Element0 控制输出端，Element1 控制输入端，同时每个 Element 又分为 Input Scope 和 Output Scope。如果开发者想要使用扬声器的播放声音功能，那么必须将这个 Unit 的 Element0 的 OutputScope 和 Speaker 进行连接。而如果开发者想要使用麦克风的录音功能，那么必须将这个 Unit 的 Element1 的 InputScope 和麦克风进行连接。使用扬声器的代码如下：

    OSStatus status = noErr;
    UInt32 oneFlag = 1;
    Uint32 busZero = 0;             // Element 0
    status = AudioUnitSetProperty(remoteIOUnit,
                                  kAudioOutputUnitProperty_EnableIO,
                                  kAudioUnitScope_Output,
                                  busZero,
                                  &oneFlag,
                                  sizeof(oneFlag));
    CheckStatus(status, @"Could not Connect to Speaker", YES);

上面这段代码就是把 RemoteIO Unit 中 Element0 的 OutputScope 连接到 Speaker 上

    static void CheckStatus(OSStatus status, NSString* message, BOOL fatal) {
      if (status != noErr) {
        char fourCC[16];
        *(UInt32 *)fourCC = CFSwapInt32HostToBig(status);
        fourCC[4] = '\0';
        if (isprint(fourCC[0]) && isprint(fourCC[1]) && isprint(fourCC[2]) && isprint(fourCC[3]))
          NSLog(@"%@: %s", message, fourCC);
        else 
          NSLog(@"%@: %d", message, (int)status);
        if (fatal) exit(-1);
      }
    }

连接成功后，就应该给 AudioUnit 设置数据格式（ASBD）了

    AudioUnitSetProperty(remoteIOUnit, kAudioUnitProperty_StreamFormat, 
                         kAudioUnitScope_Output, 1, &asbd, sizeof(asbd));

接下来我们看下如何控制输入，在 K 歌应用的场景中，会采集到用户的人声处理之后且立即给用户一个耳返（将声音在 50ms 之内输出到耳机中，让用户可以听到），那么如何让 RemoteIO Unit 利用麦克风采集出来的声音，经过中间效果器处理，最终输出到 Speaker 中播放给用户

![img](../img/audio_unit_capture_process_output_procedure.webp)

如上图所示，首先要知道数据可以从通道中传递是由最右端 Speaker（RemoteIO Unit）来驱动的，它会向它的前一级 AUNode 去要数据（图中序号 1），然后它的前一级会继续向上一级节点要数据（图中序号 2），最终会从我们的 RemoteIO Unit 的 Element1（即麦克风）中取得数据，这样就可以将数据按照相反的方向一级一级地传递下去，最终传递到 RemoteIOUnit 的 Element0（即 Speaker，图中序号 6）就可以让用户听到了

当然这时候你可能会想离线处理的时候是不能播放的，那么应该由谁来驱动呢？其实在离线处理的时候，应该使用 Mixer Unit 这个大类型下面的子类型为 Generic Output 的 Audio Unit 来做驱动端。那么这些 AudioUnit 或者说 AUNode 是如何进行连接的呢？有两种方式，第一种是直接将 AUNode 连接起来；第二种是通过回调把两个 AUNode 连接起来

1.  直接连接的方式
    
        AUGraphConnectNodeInput(mPlayerGraph, mPlayerNode, 0, mPlayerIONode, 0);
    
    这段代码把 Audio File Player Unit 和 RemoteIO Unit 连接起来了，当 RemoteIO Unit 需要播放数据的时候，就会调用 AudioFilePlayer Unit 来获得数据，最终数据数据会传递到 RemoteIO 中播放

2.  回调的方式
    
        AURenderCallbackStruct renderProc;
        renderProc.inputProc = &inputAvailableCallback;
        renderProc.inputProcRefCon = (__bridge void *)self;
        AUGraphSetNodeInputCallback(mGraph, ioNode, 0, &renderProc);
    
    当这个 RemoteIO Unit 需要数据输入的时候就会回调这个回调函数，该回调函数的实现如下
    
        static OSStatus renderCallback(void *inRefCon, AudioUnitRenderActionFlags *ioActionFlags,
                                       const audiotimeStamp *inTimeStamp, UInt32 inBusNumber,
                                       UInt32 inNumberFrames, AudioBufferList *ioData) {
          OSStatus status = noErr;
          __unsafe_unretained AUGraphRecorder *THIS = (__bridge AUGraphRecorder *)inRefCon;
          AudioUnitRender(THIS->mixerUnit, ioActionFlags, inTimeStamp, 0, inNumberFrames, ioData);
          return status;
        }
    
    这个回调函数主要做两件事情，第一件事情是去 Mixer Unit 里面要数据，得到数据之后放入 ioData 中，也就填充了回调方法中的数据，从而实现了 Mixer Unit 和 RemoteIO Unit 的连接


<a id="org5683819"></a>

### 小结

-   AVAudioPlayer：如果你要直接播放一个本地音频文件（无论是本地路径还是内存中的数据），使用 AVAudioPlayer 会是最佳选择
-   AVPlayer：如果是普通网络协议（HTTP、HLS）音频文件要直接播放，使用 AVPlayer 会是最佳选择
-   AudioQueue：如果你的输入是 PCM（比如视频播放器场景、RTC 等需要业务自己 Mix 或者处理 PCM 的场景），其实使用 AudioQueue 是合适的一种方式
-   AudioUnit：如果需要构造一个复杂的低延迟采集、播放、处理的音频系统，那么使用 AudioUnit （实际实现可能是使用 AUGraph 或者 AVAudioEngine 框架）会是最佳选择


<a id="org33b8af6"></a>

# Share

