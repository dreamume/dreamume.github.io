
# Table of Contents

1.  [Algorithm](#orgfbcf99a)
2.  [Review](#org18f2424)
    1.  [简介](#org7b60c82)
3.  [Tips](#org81b75ee)
4.  [Share](#orgaf2aed0)
    1.  [概述](#org33ee05d)
        1.  [批量数据聚集和预处理流水](#orgacb3f3b)
        2.  [反转索引](#org8b3fefb)


<a id="orgfbcf99a"></a>

# Algorithm

leetcode 673: <https://leetcode.com/problems/number-of-longest-increasing-subsequence/>

<https://medium.com/@dreamume/leetcode-673-number-of-longest-increasing-subsequence-5d05faa85070>


<a id="org18f2424"></a>

# Review

Cheap Paxos

<https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/web-dsn-submission.pdf>


<a id="org7b60c82"></a>

## 简介

状态机实现包含描述一个状态机系统把一系列客户命令作为输入，产生一系列状态和输出。状态机通过一系列服务器实现。它减少了实现分布式系统的问题，通过服务器来选择命令系列。


<a id="org81b75ee"></a>

# Tips

-   把精力放在更重要的事情上，提高效率
-   对知识结构进行分层，区别处理，并要形成体系化
-   对相关知识要形成深刻印象，不用刻意记忆也不会遗忘，用时自然会想起，最好（看一些经典的书的感受）
-   有些知识点不需记忆，有印象用时能快速搜索到即可


<a id="orgaf2aed0"></a>

# Share

<https://blog.twitter.com/engineering/en_us/a/2014/building-a-complete-tweet-index.html>

构建一个完整的tweet索引

目的：通过案例熟悉了解系统设计

今天，我们荣幸地宣布Twitter已索引自2006年以来的Tweet条目。

第一个Tweet条目还在八年以前，每天数千亿条Tweet描述了人们的经历和主要的历史事件。我们的搜索引擎在实时新闻和事件查询中表现优异，我们的搜索索引基础设施能对应上近期的强抽象。但我们的长期目标是让用户能够搜索发布的每一条Tweet。

新的基础设施提供复杂的搜索结果，整个TV和体育赛季、会议(#TEDGlobal)、工业讨论(#MobilePayments)，地点，商业和长期活跃的tashtag话题会话，比如日本地震，电子2012，苏格兰裁决，香港，弗格森等等。这些改变将在近期提供给用户。

在本编文章中，我们描述我们如何构建一个搜索服务有效地索引近万亿文档并服务查询平均延迟在100ms以内。

设计中最重要的因素如下：

-   模块化：Twitter已有一个实时索引（一个反转索引包含一个星期的Tweet）。我们共享代码并在这两种索引间测试，这样可以在更少的时间内创建一个清洁的系统。
-   可扩展性：全索引比实时索引大100倍，并已每周数十亿Tweet条目的速度增长。我们固定大小的实时索引集群扩展比较困难；增加容量需要重新分区并相应的操作成本。我们需要一个系统能平滑扩展。
-   有效的成本：我们的实时索引全存储在内存中来达到低延迟和快速更新。然而，使用相同的内存技术对全索引来说过于昂贵
-   简单的接口：为扩展性分区不可避免。但我们想要一个简单的接口隐藏分区细节，这样内部客户可把集群看作一个节点
-   迭代发展：索引每个Tweet的目标不可能在一个季度中达到。全索引基于之前的基础项目。在2012年，我们构建了一个大约二十亿top tweet的小历史索引，开发了一个离线数据簇和预处理流水线。2013年，我们因为规模扩展了索引，评估并调节SSD性能。2014年，我们构建了多层的全索引架构，主要工作在扩展性和操作性上


<a id="org33ee05d"></a>

## 概述

系统包含4个主要部分：一个批量数据聚集和预处理流水线；一个反转索引构建器；Earlybird分片和Earlybird根。


<a id="orgacb3f3b"></a>

### 批量数据聚集和预处理流水

实时索引的流水线每次处理一个个人的Tweets。相反的是，全索引使用批量处理流水线，每个批量是一天的Tweets。我们想要我们的离线批量处理任务与我们的实时流水线共享尽量多的代码，并且保持高效。

为此，我们打包相关实时代码进入Pig用户定义函数，这样我们可以在Pig任务中重使用它，并创建一个Hadoop任务的流水线来聚集数据和预处理Tweets。

该流水线如下：

![img](./img/building_a_completetweetindex95.thumb.1280.1280.png)

-   吸引力聚集器：统计某天每个Tweet的吸引度。这些统计将作为给Tweet打分的输入
-   聚集器：按Tweet ID把多个数据源集合到一起
-   吸收器：执行不同类型的预处理 - 语言识别，token化，文字特征萃取，URL解决方案及更多
-   得分：基于特征的吸收萃取计算得分。对更小的历史索引，该得分决定哪个Tweet将被选中进索引
-   分区：用hash算法分割数据为更小的簇。最终的输出是存储到HDFS

该流水线设计为运行一天的Tweets。我们建立该流水线来增量运行每天的处理数据。有两个好处。它允许我们用新数据增量更新索引而不需要频繁完全重建。而每天的处理又完全独立，流水线可以在Hadoop上大量并行。这允许我们定期有效重建全索引（添加新的索引字段或改变token化）。


<a id="org8b3fefb"></a>

### 反转索引

每日数据聚集和预处理工作输出每一个Tweet一个记录。输出已token化，但未反转。

