---
layout:     post
title:      "Weekly 075"
subtitle:   "Algorithm: Cherry Pickup; Review: Serving Facebook Multifeed; Tips: Avoiding Double Payments in a Distributed Payments System; Share: The Google File System"
thumbnail-img: ""
date:       2023-05-17 12:30
author:     "dreamume"
tags: 		[it]
category:   it
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# Table of Contents

1.  [Algorithm](#org63b4315)
2.  [Review](#org99e07da)
    1.  [Disaggregation 的强大](#orge1c5bd4)
    2.  [多流构建块](#orge6c7c05)
    3.  [多流，老方式：聚集设计](#org96067d8)
    4.  [Disaggregate 多流设计和实验](#orgc701607)
3.  [Tips](#orgc16f2eb)
    1.  [背景](#org217c9cc)
    2.  [幂等是什么？](#orge21ecca)
    3.  [问题状态](#org06d5ffb)
    4.  [解决方案的解释](#org2a639bd)
    5.  [保持数据库提交到最小](#org7bf2a73)
    6.  [用 Java Lambda 来恢复](#orgb03b8ae)
    7.  [处理异常 - 重试或不重试](#org7916412)
    8.  [客户端扮演一个重要角色](#org62fba6c)
    9.  [如何选择一个幂等键](#orgb93928a)
4.  [Share](#org496e75d)
    1.  [简介](#org01b8ca3)
    2.  [设计总览](#org128d71c)
        1.  [假设](#org6cb9a52)
        2.  [接口](#org6570470)
        3.  [架构](#orgf14d967)
        4.  [单个主服务器](#orgc8edec1)
        5.  [块大小](#org28e556b)
        6.  [Metadata](#orgb5b0f2a)
        7.  [一致性模型](#orgd7eb099)


<a id="org63b4315"></a>

# Algorithm

Leetcode 741: [Cherry Pickup](https://leetcode.com/problems/cherry-pickup/description/)

<https://dreamume.medium.com/leetcode-741-cherry-pickup-eddb682bbb42>


<a id="org99e07da"></a>

# Review

服务 Facebook 多流：通过重新设计获得效率和性能

<https://engineering.fb.com/2015/03/10/production-engineering/serving-facebook-multifeed-efficiency-performance-gains-through-redesign/>

![img](../img/disaggregate_multifeed.webp)


<a id="orge1c5bd4"></a>

## Disaggregation 的强大

Disaggregation 已证明对 Facebook 是一个有用的策略。分割系统为它们的核心组件和再让他们合成的想法对我们的基础设施在灵活性和可扩展性上更有意义

disaggregation 是什么意思？当工程师通常使用各种类型的服务器，每个有它自己的 CPU，内存和闪存或磁盘，每个服务器配置使用这些资源的不同比例。运行在服务器上的服务以一种固定聚合的方式使用这些资源。Disaggregation，相反地，创建特殊的服务器池，每个聚焦在一种资源类型比如计算，内存，HDD 存储或闪存等

分割系统为构建块和让这些分片适配可有多维度的优势：

-   硬件替换高效和利用高效：我们可单独升级和替换每种资源类型，有潜力地减少总的硬件替换数。另外，每种资源（比如 CPU，内存）可更好的扩展这样能更好的利用和最小化资源浪费
-   自定义配置：我们可定制存储设计，例如，针对我们的需要
-   加速新技术适配：当一个新的硬件技术变得有效，我们可快速适配
-   软件生产可靠性和性能：软件生产组件可被重新设计且每个组件可在单独的服务器池中运行。这允许软件性能提升和可靠性得到改善
-   CPU 效率：各种工作负载在同一个服务器可能不能很好工作，对内核或处理器来说管理工作负载来获得高 CPU 利用率很困难

一个例子，我们均衡 disaggregation 的概念来重新设计多流，一个包含在新闻流里的分布式后台系统。当一个人获取他的 Facebook 流，多流查找用户的朋友，找到所有他们的最新动作，且决定基于某个关系和排名算法下如何渲染。disaggregation 的结果关系到基础设施跨越多个方面的追踪：

-   高效：多流 aggregator 和叶子基础设施对内存和 CPU 消耗优化的 40% 效率改进
-   性能：10% 的多流 aggregator 延时缩减
-   扩张性：多流的每个组件（例如，aggregator 和叶子）可独立扩展
-   可靠性：增加流量峰值的恢复能力；组件故障（例如 aggregator 和叶子）隔离


<a id="orge6c7c05"></a>

## 多流构建块

为理解如何获得这些结果，我们应该首先分解多流主要的高层组件

-   Aggregator：查询引擎接受用户请求和从后端存储提取新闻流。它也做新闻流 aggregation，排名和过滤且返回结果给客户端。aggregator 是 CPU 密集的但内存不密集
-   叶子：分布式存储层索引大多数最近的新闻流动作和在内存中存储它们。通常 20 个叶子服务器作为一个组工作且全复制包含所有用户的索引数据。每个叶子服务从 aggregator 来的数据提取请求。每个叶子是内存密集的但 CPU 不密集
-   尾部：输入数据流水线指导用户动作和实时反馈到叶子存储层
-   持久化存储：从开始重加载一个叶子的裸日志和快照


<a id="org96067d8"></a>

## 多流，老方式：聚集设计

![img](../img/multifeed_rack_old_way.webp)

过去，每个多流 aggregator 跟一个叶子成对，且它们位于一个共享的服务器上。二十个这样的服务器在一起成组，作为一个复制节点且包含用户的新闻流数据。每个复制节点有 20 个 aggregator 和 20 个叶子。当接收到一个请求，每个 aggregator 发散请求到所有叶子来提取数据，排行和过滤数据且返回结果给客户端。我们获得多流服务器高 CPU 能力和大型内存存储。但这有一些问题：

-   可靠性：通常对一个 aggregator 可能获得一个有很大朋友的用户的一个重请求，导致 CPU 使用上的一个凸起高峰。如果峰值足够大，因为 aggregator 消耗 CPU，在相同服务器上的叶子可能变得不稳定。任何 aggregator（和它对应的服务器）和叶子交互也变得不稳定，导致复制节点一个迭代的问题出现
-   硬件可扩展性：我们的基础设施中有许多复制节点。容量配置基于 CPU 服务用户请求的需求。我们添加数百个复制节点来协调随时间增长的流量。这样，对每个 CPU 的内存增加。明显内存过度构建因为当复制节点增加时它不是必要的资源
-   资源浪费：每个尾部转发用户行为且反馈到一个叶子服务器。它是多流的实时数据流水线。叶子服务器花费 10% 的 CPU 来执行这些实时升级。我们提到的复制节点数使用不必要的 CPU 资源保持我们的叶子存储更新
-   性能：aggregator 和叶子有非常不同的 CPU 特性。aggregator 线程跟叶子线程的 CPU 缓存竞争，导致缓存冲突和资源竞争。因为很多线程运行导致高的线程切换成本


<a id="orgc701607"></a>

## Disaggregate 多流设计和实验

![img](../img/disaggregate_multifeed_design.webp)

对这些确定的问题：我们如何构建硬件和改变软件产品架构来处理这些问题？在深入调查和分析之后，我们决定实现 disaggregate 硬件/软件设计多流处理。首先，我们设计一些服务器持有密集 CPU 的能力（A 类型服务器）且一些有大内存存储（B 类型服务器）。然后我们把 aggregator 放在 A 类型服务器上且叶子放在 B 类型服务器上，这使我们能够优化线程配置，减少线程切换成本，启动更好的 NUMA 平衡，且调整 aggregator 和叶子的比例

disaggregate 设计在我们内部的实验中显示的改进：

-   优化硬件/服务使用率：通过调整 aggregator 和叶子的比例，我们可缩减总的 CPU 到内存比例从 20:20 到 20:5 或 20:4。这对内存是 75% 到 80% 的缩减
-   服务能力可扩展性：aggregator 和叶子能力可被独立扩展。这允许软件有更多的灵活性
-   性能：aggregator 的平均客户端延时降低 10%
-   可靠性：disaggregate 设计更能适应突然的写流量高峰。任意 aggregator 故障可作为独立事件，不影响其他 aggregator 和叶子

对这样的鼓舞结果，我们快速的采用了设计。我们从概念设计到最后的部署只用了几个月，且把新的 disaggregate 架构添加在现有的多流配置上。其次，我们对其他 Facebook 服务比如搜索探索 disaggregate 闪存雪橇技术，操作分析和数据库。我们乐观认为受益是显著的


<a id="orgc16f2eb"></a>

# Tips

[Avoiding Double Payments in a Distributed Payments System](https://medium.com/airbnb-engineering/avoiding-double-payments-in-a-distributed-payments-system-2981f6b070bb)


<a id="org217c9cc"></a>

## 背景

Airbnb 迁移了它的基础设施到面向服务架构（SOA）。SOA 提供许多优点，比如开启开发者指导说明和更快地迭代能力。然而，它也对票务和支付应用程序带来挑战因为它使维护数据集成更加困难。一个 API 调用一个服务进一步调用 API 到下面的服务，每个服务改变状态且可能有副作用，相当于执行一个复杂的分布式交易

为确保所有服务间的一致性，会使用一些协议比如两阶段提交。没有这样的协议，分布式交易会对数据集成维护，允许优雅的降级和取得一致性形成挑战。请求在分布式系统中也必然存在失败 - 连接丢失和在某点超时，特别对包含多个网络请求的交易

有三个不同的常用技术用于分布式系统来获得最终一致性：读修复，写修复和异步修复。每个处理有各自的好处和妥协。我们的支付系统在各种功能中使用这三个处理

异步修复包含服务器响应运行数据一致性检查，比如表扫描，lambda 函数和 cron 任务。另外，服务器到客户端的异步通知在支付中广泛使用来强制客户端的一致性。异步修复，异步通知可用于读写修复技术的结合，提供一个解决方案复杂度上对防御妥协的第二条线

在本文中我们的解决方案使用写修复，客户端到服务器的每个写调用尝试修复一个不一致，被破坏的状态。写修复需要客户端更灵活且允许重复的请求且不维护状态（除了重试）。客户端可按需请求最终的一致性，在用户体验上控制它们。幂等在实现写修复中是一个及其重要的属性


<a id="orge21ecca"></a>

## 幂等是什么？

一个 API 请求是幂等的，客户端可重复调用并获得相同的结果。即多次重复的请求效果相同，如同一个请求一样

这个技术在票务和支付系统包括资金转移里普遍使用，支付请求只完整处理一次（也成为只转发一次）。如果一个转移资金的单一操作多次调用，底层系统最多只转移一次。这样 Airbnb 支付 API 避免多次支付给宿主，甚至多次向客户收费

设计上，幂等允许一个 API 使用自动重试机制从客户端多个调用来获得最终的一致性。这个技术通常在幂等的客户端服务器关系中使用，且现在有时我们使用在我们的分布式系统中

在高层，如下的图显示一些简单的重复请求和理想的幂等行为的场景。不管收费请求有多少次，客户最多只被收费一次

![img](../img/multi_request_and_idempotent_in_airbnb_payment_system.webp)


<a id="org06d5ffb"></a>

## 问题状态

对我们的支付系统保证最终一致性是最重要的。幂等是一个想要的机制在分布式系统中达到它。在 SOA 世界里，我们将故意运行一些错误。例如，如果客户端消费响应失败如何恢复？如果响应丢失或客户端超时会怎么处理？如果竞争条件导致订阅按钮点击两次会怎么处理？我们的需求如下：

-   实现一个单一的，对我们给定案例特殊自定义处理的解决方案，我们更需要一个一般化能配置幂等的解决方案用于 Airbnb 的各种支付 SOA 服务
-   当基于 SOA 的支付产品迭代中，我们不能不能在数据一致性上妥协因为这将直接影响我们的社区
-   我们需要非常低的延迟，这样构建一个分离，单独的幂等服务不能低效。最重要的，服务将遭遇原本就要解决的相同问题
-   Airbnb 使用 SOA 扩展它的工程师组织，让每个开发者对数据集成和最终一致性挑战特殊化是低效的。我们想要产品开发者屏蔽这些困扰并让他们集中于产品开发和快速迭代

另外，对代码可读性，可测试性和解决问题能力的妥协都被认为是不可能取得成功的


<a id="org2a639bd"></a>

## 解决方案的解释

我们想要能够唯一确认每个来的请求。另外，我们需要精确跟踪和管理特别请求的生命周期

我们在多个支付服务中实现和利用 Orpheus，一个一般化幂等库。Orpheus 是希腊神话中的传奇英雄，他能策划和吸引所有生物

我们选择一个库作为一个解决方案因为它提供低延时且对高速产品代码和低速系统管理代码间提供清晰地隔离。在高层，它包含如下简单的概念：

-   一个幂等键给到框架，代表一个幂等请求
-   幂等信息表，总是从一个切片主数据库中读写（为一致性）
-   数据库交易组合代码库的不同部分来确保原子性，使用 Java lambda
-   错误响应分类为可重试和非可重试

我们将详细说明幂等保证的复杂分布式系统可变成自愈和最终一致性。我们也将谈及一些妥协和我们的解决方案需要考虑到的额外复杂性


<a id="org7bf2a73"></a>

## 保持数据库提交到最小

在幂等系统中的一个关键需求是只生成两个输出，成功或失败，并确保一致性。否则，数据变种可导致数小时处理和不正确的支付。因为数据库提供 ACID 属性，数据库交易在确保一致性时可有效使用原子性写数据。一个数据库提交可保障整体为成功或失败

Orpheus 聚焦在假设大多数标准 API 请求可分割为三个不同的阶段：Pre-RPC，RPC 和 Post-RPC

一个 RPC 或原创过程调用指当一个客户端生成一个请求到一个远端服务器且等待该服务器在重置它的处理前完成请求的过程。在支付 API 的上下文中，我们把一个 RPC 作为一个网络中到下流服务的一个请求，其包括外部支付处理和请求银行。剪短地说，如下是每个阶段发生的事情：

1.  Pre-RPC：支付请求的细节记录在数据库中
2.  RPC：请求在网络中对外部服务活跃且收到响应。这是做一个或多个幂等计算或 RPC 的地方（例如，如果有重试首先查询交易状态的服务）
3.  Post-RPC：从外部服务记录响应细节到数据库，包含它的成功和是否一个坏的请求可重试

为维护数据集成，我们继承两个简单的规则：

1.  在 Pre 和 Post-RPC 阶段没有网络上的服务交互
2.  在 RPC 阶段没有数据库交互

我们需要想要避免混合数据库操作和网络通信。我们学到在 Pre 和 Post-RPC 阶段网络调用（RPC）是困难的且导致一些坏的结果比如快速连接池耗尽和性能降级。网络调用是不可靠的。因此，我们封装 Pre 和 Post-RPC 阶段在数据库交易内部被库本身初始化

我们也想要调用单个 API 请求可能包含多个 RPC。Orpheus 支持多 RPC 请求，但本文中我们想要阐述我们的想法进程只设计单 RPC 的情况

![img](../img/network_communication_kept_separate_from_database_transactions.webp)

如上图所示，每个数据库提交在每个 Pre-RPC 和 Post-RPC 阶段被组合成一个数据集交易。这确保原子性 - 工作的整体（Pre-RPC 和 Post-RPC 阶段）可作为一个整体一致性的失败或成功。这个动机是系统应该失败在它可恢复的地方。例如，如果一些 API 请求在一个长的数据库提交中失败，它会非常困难地系统性保持追踪每个失败发生的地方。注意到所有网络通信，RPC，从所有数据库交易中分离

这里一个数据库提交包含一个幂等库提交和应用程序层数据库提交，所有组合在相同的代码块中。不小心处理，在真实的代码中这将开始变得杂乱。我们也感觉它不应该是产品开发者的责任来调用某个幂等函数


<a id="orgb03b8ae"></a>

## 用 Java Lambda 来恢复

Java lambda 表达式可无缝组合多个句子为一个数据库交易，而不影响可测试性和代码可读性

如下是一个例子，在行为中用 Java 简单使用 Orpheus

    public Response processPayment(InitiatePaymetRequest request, UriInfo uriInfo)
        throw YourCustomException {
        return orpheusMamager.process(request.getIdempotencyKey(),
                                      uriInfo,
                                      // 1. Pre-RPC
                                      () -> {
                                          // Record payment request information from the request object
                                          PaymentRequestResource paymentRequestResource = recordPaymentRequest(request);
                                          return Optional.of(paymentRequestResource);
                                      },
                                      // 2. RPC
                                      (isRetry, paymentRequest) -> {
                                          return executePayment(paymentRequest, isRetry);
                                      },
                                      // 3. Post RPC - record response information to database
                                      (isRetry, paymentResponse) -> {
                                          return recordPaymentResponse(paymentResponse);
                                      });
    }

在更深一层，有一个源码的简单摘录

    public <R extends Object, S extends Object, A extends IdempotencyRequest> Response process(
                                                                                               String idempotencyKey,
                                                                                               UriInfo uriInfo,
                                                                                               SetupExecute<A> preRpcExecutable, // Pre-RPC lambda
                                                                                               ProcessExecutable<R, A> rpcExecutable, // RPC lambda
                                                                                               PostProcessExecutable<R, S> postRpcExecutable) // Post-RPC lambda
        throws YourCustomException {
        try {
            // Find previous request (for retries), otherwise create
            IdempotencyRequest idempotencyRequest = createOrFindRequest(idempotencyKey, apiUri);
            Optional<Response> responseOptional = findIdempotencyResposne(idempotencyRequest);
    
            // Return the resposne for any deterministic end-states, such as
            // non-retryable errors and previously successful responses
            if (responseOptional.isPresent()) {
                return responseOptional.get();
            }
    
            boolean isRetry = idempotencyRequest.isRetry();
            A requestObject = null;
    
            // STEP 1: Pre-RPC phase:
            // Typically used to create transaction and related sub-entities
            // Skipped if request is a retry
            if (!isRetry) {
                // Before a request is made to the external service, we record
                // the request and idempotency commit in a single DB transaction
                requestObject = 
                    dbTransactionManager.execute(
                                                 tc -> {
                                                     final A preRpcResource = preRpcExecutable.execute();
                                                     updateIdempotencyResource(idempotencyKey, preRpcResource);
    
                                                     return preRpcResource;
                                                 });
            } else {
                responseObject = findResponseObject(idempotencyRequest);
            }
    
            // STEP 2: RPC phase:
            // One or more network calls to the service. May include
            // additional idempotency logic in the case of a retry
            // Note: NO database transactions should exist in this executable
            R rpcResponse = rpcExecutable.execute(isRetry, requestObject);
    
            // STEP 3: Post-RPC phase:
            // Response is recorded and idempotency information is updated,
            // such as releasing the lease on the idempotency key, Again,
            // all in one single DB transaction
            S response = dbTransactionManager.execute(
                                                      tc -> {
                                                          final S postRpcResponse = postRpcExecutable.execute(isRetry, rpcResponse);
                                                          updateIdempotencyResource(idempotencyKey, postRpcResponse);
    
                                                          return postRpcResponse;
                                                      });
    
            return serializeResponse(response);
        } catch (Throwable exception) {
            // If CustomException, return error code and resposne based on
            // 'retryable' or 'non-retryable'. Otherwise, classify as 'retryable'
            // and return a 500.
        }
    }

这些隔离提供一些妥协。开发者必须使用预先考虑来确保代码可读性和可维护性作为新开发者持续贡献。他们也需要一致评估适合的依赖和数据。API 调用现在需要重构为三个更小的代码块，其限制开发者写代码的自由度。它事实上对一些复杂 API 调用高效分离成三个阶段比较困难。我们的一个服务实现了有限状态机，对每个交易用 StatufulJ 作为一个幂等的阶段，你可安全地在一个 API 调用中多次幂等调用


<a id="org7916412"></a>

## 处理异常 - 重试或不重试

对 Orpheus 框架，服务器应该知道一个请求是否可安全重试及什么时候不行。当情况出现时，异常需要被小心地处理 - 它们应该被分类为可重试或不可重试。这对开发者添加了一层复杂度且如果没有很好地处理将产生坏的影响

例如，假设一个下载服务临时故障，但异常抛出错误的标签为不可重试而实际上应该为可重试。该请求被定义为失败，且后续重试请求将返回不可重试的错误。相反地，如果一个异常被标签为可重试当它实际上应该为不可重试，则导致双花并需要人工干预

一般的，我们相信未知运行时异常由于网络和基础设施问题（5XX HTTP 状态）为可重试。我们期望这些错误是暂时的，且我们期望后续的重试最终会成功

我们分类验证错误，比如无效的输入和状态（比如，你不能退还一笔退款），不可重试（4XX HTTP 状态） - 我们期望同一请求的所有后续重试在相同的状态下失败。我们创建一个自定义，一般化异常类处理这些情况，缺省为不可重试，且对某些其他情况，分类为可重试

请求负载对每个请求仍然是相同的且不能修改，否则它违背了幂等请求的定义

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">可重试</th>
<th scope="col" class="org-left">不可重试</th>
</tr>
</thead>

<tbody>
<tr>
<td class="org-left">临时的 - 我们期望后续请求得到不同的结果</td>
<td class="org-left">我们期望后续请求也会失败</td>
</tr>


<tr>
<td class="org-left">内部的服务器错误</td>
<td class="org-left">无效的输入和请求</td>
</tr>


<tr>
<td class="org-left">数据库或网络连接问题</td>
<td class="org-left">行为不支持</td>
</tr>


<tr>
<td class="org-left">5XX HTTP 状态</td>
<td class="org-left">4XX HTTP 状态</td>
</tr>
</tbody>
</table>

还有更多的模糊边界情况需要小心处理。例如，一个 null 值从数据库返回由于连接问题导致，其跟从客户端或三方响应返回的不同


<a id="org62fba6c"></a>

## 客户端扮演一个重要角色

如同在文章开始所提及的，客户端在一个写修复系统中必须更智能。当与一个幂等库如 Orpheus 交互时它必须拥有一些关键责任：

-   对每个新请求用一个唯一的幂等键；重试时使用相同的幂等键
-   在调用服务之前持久化这些幂等键到数据库
-   适时消费成功的响应并消除幂等键
-   确保在重试时对请求负载的修改是不允许的
-   基于业务需要小心设计和配置自动重试策略（使用指数回退或随机等待时间来避免突暴问题）


<a id="orgb93928a"></a>

## 如何选择一个幂等键

选择一个幂等键是至关重要的 - 客户端可基于使用的键选择要么请求水平的幂等或条目水平的幂等。这个决定基于不同的业务情况而不同，但请求水平幂等是最直接和常见的

对请求水平的幂等，一个随机且唯一的键应该从客户端选择为了确保在条目收集水平上的幂等。例如，如果我们想要允许对保留订购（比如小的预先支付）有多个不同的支付，我们需要确保幂等键是不同的。使用 UUID 格式是一个好例子

条目水平幂等比请求水平幂等更严谨和受限制。如果我们想要确保一个给定 ID 1234 的一个给定 $10 的支付一次只能退回 $5，因为我们可技术上使得 $5 请求为两次。我们然后想要使用一个确定性的幂等键基于条目模型来确保条目水平的幂等。一个例子格式为 payment-1234-refund。每个退回请求对一个唯一的支付在条目水平上（1234 支付）为幂等了


<a id="org496e75d"></a>

# Share

[The Google File System](https://pdos.csail.mit.edu/6.824/papers/gfs.pdf)

我们已经设计和实现了 Google 文件系统，用于大型分布式数据密集应用程序的可扩展分布式文件系统。它提供运行在廉价商业硬件上的容错，转发高聚集性能给大量客户端

当如以前的分布式文件系统一样共享许多相同的目标，我们的设计由我们的应用程序负载和技术环境观测驱动，当前的及参与的，反应了跟一些早期文件系统假设的标志性分离。这使得我们重新检查传统的选择并探索激进的不同设计点

文件系统成功满足了我们的存储需求。它在 Google 中作为存储平台广泛应用，它使用我们的服务包含需要大型数据集的搜索和开发效能来产生和处理数据。最大的簇提供跨越超过一千台机器上数千个磁盘数百 TB 数据的存储，被数百个客户端并行访问

在本文中，我们呈现文件系统接口扩展设计来支持分布式应用程序，讨论我们设计的许多方面，且报道微观评测和实际使用上的度量


<a id="org01b8ca3"></a>

## 简介

我们设计实现了 Google 文件系统（GFS）来满足 Google 数据处理的快速增长需求。GFS 共享许多跟之前的分布式文件系统相同的目标，比如性能，可扩展性，可靠性和有效性。然而，它的设计由我们的应用程序工作负载和技术环境的关键观测驱动，包含当前的和参与的，反应到跟某些早期文件系统设计假设的标志性不同。我们重新检查传统选择和探讨在设计空间上的激进的不同点

首先，组件故障是规范而不是异常。文件系统包含由廉价商业部件组成的数千个存储机器和被大量客户端机器访问。组件的数量和质量虚拟保证一些在任意时刻会不能工作且一些不能从它们当前的故障中恢复。我们看到由应用程序、操作系统 Bug，人为操作和磁盘故障，内存，连接器，网络和电力供应等引起的问题。因此，常量观测，错误检测，容错和自动恢复必须集成进系统中

其次，传统标准下文件很大。数 GB 的文件很常见。每个文件典型地包含许多应用程序对象比如 Web 文档。当我们与快速增长的数 TB 级数据集工作包含数十亿个对象，管理数十亿个大约 KB 级别文件是笨拙的，即使文件系统支持它。结果，设计假设和参数比如 I/O 操作和块大小不得不重访问

第三，多数文件已添加新数据而不是修改现存数据的方式改动。在文件中随机写实际上不存在。一旦写入，文件是只读的，且经常是串行读。各种数据共享这些特征。一些包含大型仓库数据分析程序会搜索。一些为数据流被应用程序持续产生。一些为压缩数据。一些为一台机器生产的中间结果且在另一台上处理，有可能同时也可能延迟。给定这些大文件的访问范型，添加变成性能优化的核心且原子化保证，而缓存数据在客户端阻塞它的处理

第四，共存的应用程序和文件系统 API 增加整个系统的灵活性。例如，我们释放 GFS 一致性模型来简化文件系统，而在应用程序上不引入过于复杂的负担。我们也引入一个原则添加操作这样多个客户端可同时添加到文件不需要额外的同步操作。这些将在后续章节详细讨论

多个 GFS 簇为不同的目的采用。最大的有超过 1000 个存储节点，超过 300 TB 的磁盘存储和在连续基础上数百个不同机器上客户端负重访问


<a id="org128d71c"></a>

## 设计总览


<a id="org6cb9a52"></a>

### 假设

在设计我们需要的文件系统时，我们通过假设提供挑战和机会来引导。我们提及一些关键观测和更详细地展示我们的假设

-   系统构建在许多经常故障的廉价商业组件上。它可常量观测自身且检测，容错和用日常常规来恢复组件故障
-   系统存储适中数量的大文件。我们期望数百万个文件，每个典型的 100 MB 或更大。几 GB 的文件是常见的且应该被高效管理。小文件必须支持，但我们不需要优化它
-   工作负载主要包含两类读：大流读和小的随机读。在大流读中，单个操作典型地读数百个 KB，更通常的是 1 MB 或更多。相同客户端的成功操作通常读取文件的一个连续范围。一个小的随机读典型地读取某偏移下的几 KB。性能敏感的应用程序通常批量且排序它们的小的读取来提升文件的稳定而不是来回读
-   工作负载也有许多大的顺序写数据添加到文件。典型的操作大小跟那些读相似。一旦写入，文件很少修改。在文件中任意位置进行小写入是支持的但不高效
-   系统必须高效实现对多个客户端并行添加内容到文件有良好的语义。我们的文件通常作为生产者消费者队列或多方合并。数百个生产者，每个机器运行一个，将并行添加内容到文件。最小的同步原子化是必须的。文件可延后读，或一个消费者可能同时读文件
-   高带宽支持比低延迟更重要。我们的多数目标应用程序在高速处理数据上有要求，而少量对单个读写有严格的响应时间要求


<a id="org6570470"></a>

### 接口

GFS 提供一个相似的文件系统接口，虽然它没有实现比如 POSIX 那样的标准 API 接口。文件以目录分层组织且由路径名确定。我们支持常见的创建，删除，打开，关闭，读写文件操作

更进一步，GFS 有快照和记录添加操作。快照以廉价方式创建一个文件目录树的备份。记录添加允许多个客户端并行添加数据到相同的文件同时保证每个独立客户端添加的原子性。这用来实现多路合并结果和生产者消费者队列使得许多客户端可在无锁情况下同时添加。我们发现这些类型文件在构建大型分布式应用程序时非常有用。快照和记录添加将在后续章节讨论


<a id="orgf14d967"></a>

### 架构

一个 GFS 簇包含一个主和多个块服务器，被多个客户端访问。如下图所示。通常是典型的一个商业 Linux 机器运行用户级服务器进程。在同一台机器上运行块服务器和客户端也是很容易的，如果机器资源允许且由运行可能的较脆弱应用程序代码可接受导致的更低的可靠性

![img](../img/gfs_architecture.png)

文件被分割成固定大小的块。每块由一个不可修改且全局唯一的 64 位块柄确定，其由主服务器在块创建的时间点上给出。块服务器存储块在本地磁盘上作为 Linux 文件和通过一个块句柄和字节范围读写块数据。对可靠性，每个块复制在多个块服务器上。缺省的，我们存储三个复制节点，用户可为文件命名空间的不同区域指定不同的复制级别

主服务器维护所有文件系统 metadata。这包括命名空间，访问控制信息，从文件映射到块，且块的当前位置。它也控制系统级别活动比如块租赁管理，孤儿块垃圾收集和块服务器之间块迁移。主服务器定期用心跳信息与每个块服务器通信来给出它的指令和收集它的状态

GFS 客户端代码连接每个应用程序实现文件系统 API 和主服务器与块服务器通信来在应用程序收益上读写数据。客户端在 metadata 操作上与主服务器交互，但所有数据容忍通信直接到块服务器。我们不提供 POSIX API 且因此不需要 hook 到 Linux vnode 层

客户端和块服务器都不缓存文件数据。客户端缓存提供的收益很少因为多数应用程序通过大文件流或工作集太大而不能缓存。通过消除缓存一致性问题它们简化客户端和系统（客户端不缓存 metadata）。块服务器不需要缓存文件数据因为块作为本地文件存储且 Linux 的缓存区缓存以保持在内存中频繁访问数据


<a id="orgc8edec1"></a>

### 单个主服务器

单个主服务器极大简化了我们的设计和启动主服务器来使复杂块位置和复制决定使用全局信息。然而，我们必须在读写上最小化包含这样它不会变成瓶颈。客户端不通过主服务器读写文件数据。客户端询问主服务器它应该联系哪个块服务器。它缓存这个信息在限定时间内且对许多顺序操作直接与块服务器交互

让我们对上图的读交互作一个简单的解释。首先，使用固定块大小，客户端传输一个文件名和应用程序在文件块索引中指定的字节偏移。然后，它发送给主服务器一个请求包含文件名和块索引。主服务器用块句柄和复制节点位置来响应。客户端缓存这个信息使用文件名和块索引作为键

客户端然后发送一个请求到一个复制节点，通常是最接近的一个。请求指定块句柄和块中字节范围。后续相同块的读不再需要客户端主服务器交互直到缓存信息过期或文件被重新打开。事实上，客户端典型地在请求中询问多个块且主服务器也包含这些块的信息给它们。这个额外信息使得一些将来的客户端和主服务器交互在实际上没有额外的成本


<a id="org28e556b"></a>

### 块大小

块大小是关键设计参数之一。我们选择 64 MB，其比典型的文件系统块大小更大。每个块复制节点在块服务器上作为一个普通 Linux 文件存储且按需扩展。延迟空间分配避免由于内部碎片化而浪费空间，也许这是对大块最大的阻碍因素

一个大块提供几个重要的优势。首先，它减少客户端跟主服务器交互的需要因为读写相同的块只需要一个初始化请求来定位块信息。该缩减对我们的工作负载作用明显因为应用程序大多数串行读写大文件。甚至对小的随机读，对多 TB 工作集客户端可舒适地缓存所有块位置信息。其次，因为一个大块，一个客户端更可能在一个给定块上执行多次操作，它可通过保持一个在扩展时期持久化的到块服务器的 TCP 连接缩减网络负载。第三，它缩减主服务器上 metadata 存储的大小，其带来的好处我们在之后的章节会提及

另一方面，一个大块及延迟空间分配，有它的缺点。一个小文件包含少量块，可能是一个。块服务器存储这些块可能成为蜜罐如果许多客户端访问相同的文件。实际上，蜜罐不是一个主要的问题因为我们的应用程序多数情况下顺序读多个大块文件

然而，蜜罐当 GFS 在批处理队列系统中首次使用时：一个执行作为一个单块文件写入 GFS 且然后同时在数百台机器上开始。一些块服务器存储这个执行被数百个同时发起的请求加载。我们修复这个问题通过存储这样的执行以更高的复制因子并使得批处理队列系统随机开始应用程序开始时间。一个潜在的长期解决方案是允许客户端在这种情况下从其他客户端读取数据


<a id="orgb5b0f2a"></a>

### Metadata

主服务器存储三种主要的 metadata：文件和块命名空间，文件到块的映射和每个块复制节点的位置。所有的 metadata 在主服务器内存中。头两种类型（命名空间和文件到块映射）也通过操作日志记录修改持久化保存在主服务器磁盘和复制在远程机器。使用一个日志允许我们来简单可靠地升级主服务器状态且不需担心主服务器崩溃导致的不一致性风险。主服务器不持久化存储块位置信息。它在启动时询问每个块服务器及当一个块服务器加入簇时

1.  内存数据结构

    因为 metadata 在内存中存储，主服务器操作非常快。更进一步，它容易高效对主服务器在后台周期性扫描它的整个状态。这个周期性扫描用来实现块垃圾回收，当块服务器故障时重复制和块迁移来平衡负载和块服务器间磁盘空间限制
    
    一个潜在的隐患是块数和整个系统被主服务器内存容量限制。实际上这不是一个严重的限制。主服务器每 64 MB 块维护一个少于 64 字节的 metadata。多数块是满的因为多数文件包含许多块，只有最后一个是部分满。相似地，文件命名空间数据对每个文件需要少于 64 字节因为它存储文件名使用前缀压缩
    
    如果需要支持甚至更大的文件系统，添加额外的内存到主服务器的成本相对于得到的简化，可靠性，性能和灵活性来说是一个小的代价

2.  块位置

    主服务器不持久化块服务器的块记录。它在启动时询问块服务器这些信息。主服务器可保持更新因为它控制所有块的位置和用心跳消息监控块服务器状态
    
    我们最开始尝试在主服务器上持久化保存块位置信息，但我们决定在启动时从块服务器查询数据更简单。这消除了主服务器和块服务器同步问题，块服务器加入离开簇，改变名字，故障，重启等等。在一个簇中有数百台服务器，这些事件经常发生
    
    理解这个设计决定的另一个方式是意识到一个块服务器决定它有什么块。没有必要维护一个一致性信息在主服务器上因为块服务器会导致一个块突然消失或一个操作会重命名一个块服务器

3.  操作日志

    操作日志包含关键 metadata 改变的历史记录。它是 GFS 的核心。它不仅持久化 metadata 记录，也作为一个逻辑时间线定义并行操作的顺序。文件和块，及它们的版本，唯一并最终由当它们被创建时的逻辑时间确定
    
    因为操作日志很重要，我们必须可靠存储并对使改变对客户端不可见直到 metadata 改变被持久化处理。否则，我们将丢失整个文件系统或最近的客户端操作甚至块服务器的存在。因此，我们在多个远程机器上复制它并只在刷新对应的日志记录到本地和远端磁盘之后才能响应客户端操作。主服务器在刷新前批量一些日志记录因此减少刷新和复制对整个系统吞吐的影响
    
    主服务器通过重放操作日志来恢复它的文件系统状态。为最小化启动时间，我们必须使日志小。当日志增长超过某个大小主服务器检测点它的状态这样它可通过加载本地磁盘上最新的检查点来恢复并只重放检查点之后的日志。检查点是一个压缩 B 树可直接通过内存压缩和使用命名空间查找并不需要额外的分析。这进一步加速恢复和改进有效性
    
    因为构建一个检查点需要花费一些时间，主服务器的内部状态是结构化的，一个新检查点可在不延迟输入修改的情况下创建。主服务器切换到一个新日志文件并在单独线程中创建新检查点。对一个有数百万个文件的簇它可在 1 分钟左右创建。当完成时，它写入本地和远端磁盘
    
    恢复只需要最新完成的检查点和后续日志文件。老的检查点和日志文件可删除，虽然我们保存一些来保障一些灾难事件发生。检查点时发生的故障不影响正确性因为恢复代码检测并跳过不完整的检查点


<a id="orgd7eb099"></a>

### 一致性模型

GFS 有一个松弛的一致性模型支持我们的高分布式应用程序但实现依然简单有效。我们现在讨论 GFS 的保障和它们对应用程序意味着什么。我们也强调 GFS 如何维护这些保障

1.  GFS 的保障

    文件命名空间的修改（例如文件创建）是原子的。它们被主服务器处理：命名空间加锁保障原子性和正确性；主服务器的操作日志定义一个全局的操作总序
    
    <table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
    
    
    <colgroup>
    <col  class="org-left" />
    
    <col  class="org-left" />
    
    <col  class="org-left" />
    </colgroup>
    <thead>
    <tr>
    <th scope="col" class="org-left">&#xa0;</th>
    <th scope="col" class="org-left">写</th>
    <th scope="col" class="org-left">记录添加</th>
    </tr>
    </thead>
    
    <tbody>
    <tr>
    <td class="org-left">顺序成功</td>
    <td class="org-left">已定义</td>
    <td class="org-left">定义多处的不一致</td>
    </tr>
    
    
    <tr>
    <td class="org-left">并行成功</td>
    <td class="org-left">一致但未定义</td>
    <td class="org-left">定义多处的不一致</td>
    </tr>
    
    
    <tr>
    <td class="org-left">故障</td>
    <td class="org-left">不一致</td>
    <td class="org-left">不一致</td>
    </tr>
    </tbody>
    </table>
    
    数据修改后文件范围状态依赖于修改类型，是否成功或失败，是否有并行修改。上表总结了结果。一个文件区域是一致的当所有的客户端将总是看到相同的数据，不管它们从哪个复制节点读取。一个区域在文件数据修改后是定义的当它是一致的且客户端将看到修改被完全写入。并行成功修改使得区域未定义但一致：所有客户端看到相同的结果，但它可能不能反应哪一个修改被写入。典型的，它包含多次修改的迁移碎片。一个失败的修改使得区域不一致（因此也未定义）：不同的客户端可在不同的时间看到不同的结果。我们描述我们的应用程序如何区分定义的区域和未定义区域。应用程序不需要进一步区分未定义区域的不同类型
    
    数据修改可为写入和记录添加。一个写入导致数据被写入到应用程序指定文件的偏移上。一个记录添加导致数据（记录）自动只添加一次即使有并发修改，GFS 选择的一个偏移。（相反，一个常规的添加只是在一个偏移写，客户端认为在文件的末尾）。偏移返回到客户端且标记包含记录的定义区域的开始。另外，GFS 可插入空白在记录之间
    
    在一系列成功修改后，修改的文件区域保证为定义的且包含最近修改的数据。GFS 通过这些达到：(a)在所有复制节点上已相同的顺序修改块 (b) 使用块版本信息来检测复制节点是否变脏因为有些块服务器因为宕机丢失修改。脏复制节点向主服务器询问块位置。它们有最早的机会被垃圾收集
    
    因为客户端缓存块信息，它们可在信息被刷新之前从一个脏复制节点读取。这个窗口被缓存条目的超时时间限制，且下一次打开文件，从文件缓存所有块信息。更进一步，因为多数文件为只添加，一个脏复制节点通常返回一个未成熟的块尾而不是脏数据。当一个读者重试且联系主服务器，它立即获得当前块位置
    
    在一个成功修改的长时间后，组件故障可仍然破坏或毁坏数据。GFS 确定故障块服务器通过主服务器和所有块服务器的常规握手且通过 checksum 探测数据破坏。一旦一个问题出现，数据快速从有效的块服务器恢复。一个块不可避免的丢失仅当在 GFS 重处理之前所有它的复制块丢失，典型地在数分钟内。对这样的情况，它变得无效，不是破坏：应用程序接收到清除错误而不是数据破坏
